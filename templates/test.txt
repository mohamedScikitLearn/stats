The previous methods become less useful when dealing with a large corpus because you’ll need to represent the tokens differently. Count Vectorizer will help us convert a collection of text documents to a vector of token counts. In the end, we’ll get a vector representation of the text data.
This becomes extremely useful when the dataframe contains a large corpus because it provides a matrix with words encoded as integers values, which are used as inputs in machine learning algorithms.
Count Vectorizer can have different parameters like stop_words that we defined above. However, keep in mind that the default regexp used by Count Vectorizer selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)
When you need to tokenize text written in a language other than English, you can use spaCy. This is a library for advanced natural language processing, written in Python and Cython, that supports tokenization for more than 65 languages.
Let’s tokenize the same Steve Jobs text but now translated in Spanish.
Although for languages like Spanish and English, tokenization will be as simple as separating by whitespace, for non-romance languages such as Chinese and Japanese, the orthography might have no spaces to delimit “words” or “tokens.” In such cases, a library like spaCy will come in handy. Here you check more about the importance of tokenization in different languages.
